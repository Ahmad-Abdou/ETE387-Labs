{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTOF8uxfGbpv"
      },
      "source": [
        "# Lab 2: GPT from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWC-ZXfrGbpz"
      },
      "source": [
        "In this lab, you will dive into the inner workings of the GPT architecture. You will walk through a complete implementation of the architecture in PyTorch, instantiate this implementation with pre-trained weights, and put the resulting model to the test by generating text. At the end of this lab, you will understand the building blocks of the GPT architecture and how they are connected.\n",
        "\n",
        "*Tasks you can choose for the oral exam are marked with the graduation cap ðŸŽ“ emoji.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cj1kG5UaGbp1"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRGq_mA6Gbp3"
      },
      "source": [
        "## Part 1: GPT architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjvG2tpQGbp3"
      },
      "source": [
        "GPT-2 was first described by [Radford et al. (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). To faithfully implement the model, one needs to also read the earlier paper by [Radford et al. (2018)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf). Another important source of information is the code released by OpenAI, which is available on GitHub ([link](https://github.com/openai/gpt-2)).\n",
        "\n",
        "The GPT architecture is made up of a stack of Transformer blocks. Each block has two main parts: one handles multi-head self-attention, and the other is a feed-forward network. Before these parts do their work, their input undergoes layer normalisation, and residual connections are added to help the model learn more effectively. The input to the architecture is a sequence of token IDs; these are turned into embeddings and augmented with information about the absolute position of each token in the sequence. The output layer converts the internal representations into logit scores for every token in the vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55-V_lotGbp4"
      },
      "source": [
        "### Model configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_v37IZGGbp4"
      },
      "source": [
        "[Radford et al. (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) present four increasingly larger GPT models based on the same architecture. Here, we will implement the smallest of these, characterised by the following hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSxxMSvDGbp5"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    n_vocab: int = 50_257\n",
        "    n_ctx: int = 1024\n",
        "    n_embd: int = 768\n",
        "    n_head: int = 12\n",
        "    n_layer: int = 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9-wRV25Gbp5"
      },
      "source": [
        "#### ðŸŽˆ Task 2.01: Model configuration\n",
        "\n",
        "Explain the purpose of these hyperparameters. In particular, where does the number 50,257 come from?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `n_vocab` is the numbner of unique tokens, the number 50_257 came from the tokenizer when the model was trained.</br>\n",
        "- `n_ctx` is the context length which is the number of tokens that the model can process at once. </br>\n",
        "- `n_embd` the number of dimensions for the embedded vectors. </br>\n",
        "- `n_head` the number of multihead self attention (MHA).</br>\n",
        "- `n_layer` The number of transformer blocks taht contains MHA, FFN."
      ],
      "metadata": {
        "id": "hI2JbKqZIGzP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgJufnRjGbp5"
      },
      "source": [
        "### GELU activation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYxamTzLGbp5"
      },
      "source": [
        "We start by implementing the feed-forward network. This is a standard two-layer network with a Gaussian Error Linear Unit (GELU) activation function ([Hendrycks and Gimpel, 2016](https://doi.org/10.48550/arXiv.1606.08415)).\n",
        "\n",
        "The GELU is a smooth version of the rectified linear unit (ReLU) that weights inputs by their value under the cumulative distribution function of the standard Gaussian. This function is commonly denoted by $\\Phi$. For example, $\\text{GELU}(0{.}5) = 0{.}5 \\cdot \\Phi(0{.}5) \\approx 0{.}5 \\cdot 0{.}6915 = 0{.}3457$ because approximately 69.15% of normally distributed data lies to the left of $0{.}5$.\n",
        "\n",
        "When GPT-2 was released, computing the GELU exactly was expensive, and the released code therefore used an approximation originally presented by [Page (1977)](https://doi.org/10.2307/2346872). We follow suit here, as we want to create a replica of the original model. However, it is worth mentioning that PyTorch now offers an exact implementation of the GELU so fast that using an approximation is unnecessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzaSJjWaGbp6"
      },
      "outputs": [],
      "source": [
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + torch.tanh((2 / torch.pi) ** 0.5 * (x + 0.044715 * x**3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On5Bx62wGbp6"
      },
      "source": [
        "#### ðŸŽ“ Task 2.02: Mathematical properties of the GELU\n",
        "\n",
        "Find the minimal output value of the (approximated) GELU and the input value for which it yields that output. Use a service such as [WolframAlpha](https://www.wolframalpha.com/) for the necessary derivations. What are the main differences between the GELU and the ReLU?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "min{0.5 x (1 + tanh((2/Ï€)^0.5 (x + 0.044715 x^3)))}â‰ˆ-0.170041 at xâ‰ˆ-0.752461\n",
        "\n",
        "The difference between GELU and ReLU:\n",
        "The way they both are visualized since ReLU shows a a convergence at 0 where values less than 0 are non differentiable, meanwhile GELU shows smoothness that looks like a curve and it is differentiable across the entire x axis.\n",
        "With differentiable we mean that the model can learn and update parameters, so when it is always 0 it means nothing is learned.\n"
      ],
      "metadata": {
        "id": "ty6Bo2aDo2Lt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvmXolIcGbp6"
      },
      "source": [
        "### Feed-forward network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXwl10L6Gbp6"
      },
      "source": [
        "Next, here is the code for the feed-forward network. Note that we follow the released code and use the name **multi-layer perceptron (MLP)** rather than â€œfeed-forward networkâ€."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LARxoPLdGbp7"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, config.n_embd * 4)\n",
        "        self.c_proj = nn.Linear(config.n_embd * 4, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, n_embd = x.shape\n",
        "        # shape of x [8, 1024, 768]\n",
        "        x = self.c_fc(x)\n",
        "        # shape of x [8, 1024, 3072] because it has been multiplied by 4\n",
        "        x = gelu(x)\n",
        "        # shape of x [8, 1024, 3072] remains the same\n",
        "        x = self.c_proj(x)\n",
        "        # shape of x [8, 1024, 768] the input dimension got compressed to it is original dimension (the same as the input)\n",
        "        return x\n",
        "        # shape of x [8, 1024, 768]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "mlp = MLP(config)\n",
        "batch_size = 8\n",
        "x = torch.randn( batch_size, config.n_ctx, config.n_embd)\n",
        "x = mlp.c_fc(x)\n",
        "print(x.shape)\n"
      ],
      "metadata": {
        "id": "uS0REV66-42U",
        "outputId": "860944ea-129e-4146-8e0d-5552e042b2be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 1024, 3072])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = gelu(x)\n",
        "print(x.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "BiuNYE7nEXRl",
        "outputId": "4994603b-1cf8-47f6-f862-79eda79cb0ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 1024, 3072])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = mlp.c_proj(x)\n",
        "print(x.shape)"
      ],
      "metadata": {
        "id": "b3n9JQaSEerI",
        "outputId": "aa60b514-0cd7-4ce4-d87b-568f94fa9bc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 1024, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "038rCL_LGbp7"
      },
      "source": [
        "#### ðŸŽ“ Task 2.03: Shape annotations\n",
        "\n",
        "One of the most common errors in deep learning is a mismatch in tensor dimensions. To avoid this, it is good practice to annotate PyTorch code with shapes. For example, suppose you are given the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNJjLw-OGbp7"
      },
      "outputs": [],
      "source": [
        "f = nn.Linear(5, 7)\n",
        "x = torch.rand(2, 3, 5)\n",
        "y = f(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7vGTxjjcy_F",
        "outputId": "3fc9a00d-4b5a-4546-f550-d1f063c933af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=5, out_features=7, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Awl0OcQGbp7"
      },
      "source": [
        "The annotation of this code with shapes would look as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhRiHVKyGbp8"
      },
      "outputs": [],
      "source": [
        "f = nn.Linear(5, 7)\n",
        "# not a tensor variable; needs no annotation\n",
        "\n",
        "x = torch.rand(2, 3, 5)\n",
        "# shape of x: [2, 3, 5]\n",
        "\n",
        "y = f(x)\n",
        "# shape of y: [2, 3, 7]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ6J-unZGbp8"
      },
      "source": [
        "Annotate the shapes in the `forward()` method of the feed-forward network. Instead of using actual numbers, refer to dimension sizes by symbolic names such as `n_embd`, `batch_size` (number of samples in a batch of input data) and `seq_len` (length of an input sequence). You can introduce additional names and other notation you find useful. Make your annotations as detailed as you need them to explain how the shapes change from one line to the next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqdkI0_YGbp8"
      },
      "source": [
        "### Causal mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZbXKn0HGbp8"
      },
      "source": [
        "Our next goal is to implement the core of the GPT architecture: the multi-head attention mechanism.\n",
        "\n",
        "Recall that the attention mechanism in the Transformer decoder must be restricted to attending only to previously generated tokens. This type of attention is also called **causal attention**. In practice, we implement it through a masking technique that sets the post-softmax attention weights of future tokens to zero. The following utility function implements such a mask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8kdqhsaGbp8"
      },
      "outputs": [],
      "source": [
        "def make_causal_mask(n):\n",
        "    return torch.triu(torch.full((n, n), float(\"-inf\")), diagonal=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbSznvR4Gbp9"
      },
      "source": [
        "#### ðŸŽˆ Task 2.04: Causal mask\n",
        "\n",
        "Have a close look at the following code and run it to see the result. What are the shapes of `x` and `mask`? Given that the shapes are different, why does the addition operation in the last line not raise an error? What is the shape of the result?\n",
        "\n",
        "How does the addition operation implement masking? (Recall that the attention scores are normalised using the softmax function.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9sr9xdNGbp9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bfbe8fe-746c-486c-d6c4-046fb8715c2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.4831,   -inf,   -inf],\n",
              "          [0.0139, 0.2028,   -inf],\n",
              "          [0.6316, 0.9506, 0.6926]],\n",
              "\n",
              "         [[0.2021,   -inf,   -inf],\n",
              "          [0.5207, 0.8791,   -inf],\n",
              "          [0.3641, 0.5099, 0.9788]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "x = torch.rand(1, 2, 3, 3)\n",
        "mask = make_causal_mask(5)\n",
        "x + mask[:3, :3]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape)\n",
        "print(mask.shape)\n",
        "c_mask = x + mask[:3, :3]\n",
        "print(c_mask.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGvtTD89eoCA",
        "outputId": "99ba4866-badc-442e-c4da-150ad0224a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2, 3, 3])\n",
            "torch.Size([5, 5])\n",
            "torch.Size([1, 2, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When comparing 2 shapes, we look at their dimensions and check for 2 rules:\n",
        "\n",
        "\n",
        "1.   The 2 compared tensors must have the same dimensions size, in this case we have (3, 3) for both x and mask\n",
        "2.   One of them is 1, in here the mask is only shape of 2 so we fill the missing ones with 1 and it will be [1,1,5,5] **(broadcasting)**. However in the last line we selected only the first 3 from the mask so we get [1,1,3,3],  this will help achieving compatibiality with x [1,2,3,3], otherwise we would get an error.\n",
        "\n",
        "Now The final result was [1,2,3,3] because when comparing 2 shapes we always take the maximum size of the two input dimensions being compared.\n",
        "\n"
      ],
      "metadata": {
        "id": "_85BozsSgo2q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOv2oUSYGbp9"
      },
      "source": [
        "### Attention mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eHLIqg0Gbp9"
      },
      "source": [
        "Here is the code for the multi-head attention mechanism:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngFjO-VxGbp9"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.n_head = config.n_head\n",
        "        self.c_attn = nn.Linear(config.n_embd, config.n_embd * 3)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.register_buffer(\"mask\", make_causal_mask(config.n_ctx), persistent=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Input x:\", x.shape)\n",
        "        batch_size, seq_len, n_embd = x.shape # x.shape: (8, 1024, 768)\n",
        "        head_embd = n_embd // self.n_head # head_embd = 768 / 12 = 64\n",
        "        q, k, v = self.c_attn(x).chunk(3, dim=-1) # (8, 1024, 768) for all of them\n",
        "        print(q.shape, k.shape, v.shape)\n",
        "        q = q.view(batch_size, seq_len, self.n_head, head_embd) # (8, 1024, 12, 64)\n",
        "        k = k.view(batch_size, seq_len, self.n_head, head_embd) # (8, 1024, 12, 64)\n",
        "        v = v.view(batch_size, seq_len, self.n_head, head_embd) # (8, 1024, 12, 64)\n",
        "        print(q.shape, k.shape, v.shape)\n",
        "        q = q.transpose(-2, -3) # (8, 12, 1024, 64)\n",
        "        k = k.transpose(-2, -3) # (8, 12, 1024, 64)\n",
        "        v = v.transpose(-2, -3) # (8, 12, 1024, 64)\n",
        "        print(q.shape, k.shape, v.shape)\n",
        "        x = q @ k.transpose(-1, -2) # (8, 12, 1024, 1024)\n",
        "        print('Transpose x: ', x.shape)\n",
        "        x = x / head_embd**0.5 # (8, 12, 1024, 1024)\n",
        "        print('x / head_embd**0.5: ', x.shape)\n",
        "        x = x + self.mask[:seq_len, :seq_len] # (8, 12, 1024, 1024)\n",
        "        print('x + self.mask[:seq_len, :seq_len] x: ', x.shape)\n",
        "        x = torch.softmax(x, dim=-1) # (8, 12, 1024, 1024)\n",
        "        print('softmax x: ', x.shape)\n",
        "        x = x @ v # (8, 12, 1024, 64)\n",
        "        print('x @ v: ', x.shape)\n",
        "        x = x.transpose(-2, -3).contiguous() # (8, 1024, 12, 64)\n",
        "        print('Transpose x: ', x.shape)\n",
        "        x = x.view(batch_size, seq_len, n_embd) #(8, 1024, 768)\n",
        "        print('x.view: ', x.shape)\n",
        "        x = self.c_proj(x) # (8, 1024, 768)\n",
        "        print('self.c_proj(x): ', x.shape)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    n_vocab: int = 50_257\n",
        "    n_ctx: int = 1024\n",
        "    n_embd: int = 768\n",
        "    n_head: int = 12\n",
        "    n_layer: int = 12"
      ],
      "metadata": {
        "id": "e31OZXbs4roh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention = Attention(config)\n",
        "batch_size = 8\n",
        "x = torch.randn(batch_size, config.n_ctx, config.n_embd)\n",
        "result = attention(x)"
      ],
      "metadata": {
        "id": "cLY5gx8H2pgL",
        "outputId": "c0d02fbd-b9b0-4da8-a805-c9ddf235dc70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input x: torch.Size([8, 1024, 768])\n",
            "torch.Size([8, 1024, 768]) torch.Size([8, 1024, 768]) torch.Size([8, 1024, 768])\n",
            "torch.Size([8, 1024, 12, 64]) torch.Size([8, 1024, 12, 64]) torch.Size([8, 1024, 12, 64])\n",
            "torch.Size([8, 12, 1024, 64]) torch.Size([8, 12, 1024, 64]) torch.Size([8, 12, 1024, 64])\n",
            "Transpose x:  torch.Size([8, 12, 1024, 1024])\n",
            "x / head_embd**0.5:  torch.Size([8, 12, 1024, 1024])\n",
            "x + self.mask[:seq_len, :seq_len] x:  torch.Size([8, 12, 1024, 1024])\n",
            "softmax x:  torch.Size([8, 12, 1024, 1024])\n",
            "x @ v:  torch.Size([8, 12, 1024, 64])\n",
            "Transpose x:  torch.Size([8, 1024, 12, 64])\n",
            "x.view:  torch.Size([8, 1024, 768])\n",
            "self.c_proj(x):  torch.Size([8, 1024, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAjXBEAkGbp-"
      },
      "source": [
        "#### ðŸŽ“ Task 2.05: Multi-head attention\n",
        "\n",
        "Trace the input `x` through the `forward()` method line by line and annotate the shapes of all tensor variables. Identify all lines that rely on broadcasting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oZg9UY7Gbp-"
      },
      "source": [
        "### Layer normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CExAF7-JGbp-"
      },
      "source": [
        "As mentioned above, the inputs to both the feed-forward network and the multi-head attention mechanism undergo **layer normalisation**. This normalises the inputs to have zero mean and unit variance across the activations. [Ba et al. (2016)](https://doi.org/10.48550/arXiv.1607.06450) introduce two trainable parameters (called $\\gamma$ and $\\beta$ in the paper) that allow the network to learn an appropriate scale and shift for the normalised values.\n",
        "\n",
        "We implement layer normalisation as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg8s71m9Gbp-"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.g = nn.Parameter(torch.ones(config.n_embd))\n",
        "        self.b = nn.Parameter(torch.zeros(config.n_embd))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        variance = x.var(unbiased=False, dim=-1, keepdim=True)\n",
        "        return self.g * (x - mean) / torch.sqrt(variance + 1e-05) + self.b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4e6zXIMGbp-"
      },
      "source": [
        "#### ðŸŽˆ Task 2.06: Layer normalisation\n",
        "\n",
        "What is the relevance of the `keepdim=True` keyword argument in the `mean()` and `var()` functions? What would happen if we omitted it?\n",
        "\n",
        "What is the relevance of the constant 1e-05? What could happen if we omitted it?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`keepdim  (mean)` is used to tell to keep the last dimension in the output and dont remove it when the dimension is reduced, and this will cause mismatch when we do broadcasting since it will be one dimension less.\n",
        "\n",
        "`keepdim  (var)` is used for the same purpose.\n",
        "\n",
        "`1e-05` By adding this **epsilon** we are preventing the network from crashing since this helps avoiding division on 0. This is done by adding a very small positive number."
      ],
      "metadata": {
        "id": "ASsqnTWQ7_dW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvFFaovNGbp_"
      },
      "source": [
        "### Decoder block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GcpgRZvGbp_"
      },
      "source": [
        "We now combine the feed-forward network, the multi-head attention mechanism and the layer normalisation into a decoder block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKZ_KfGdGbp_"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config)\n",
        "        self.attn = Attention(config)\n",
        "        self.ln_2 = LayerNorm(config)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_cDPKr5Gbp_"
      },
      "source": [
        "#### ðŸŽ“ Task 2.07: Pre-norm and post-norm architectures\n",
        "\n",
        "The original Transformer ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)) is a â€œpost-norm architectureâ€, where the normalisation is applied **after** each residual block. In contrast, GPT-2 is a â€œpre-norm architectureâ€, where the normalisation is applied **before**. Find the passage in Section&nbsp;2.3 of [Radford et al. (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) that reports on this modification.\n",
        "\n",
        "[Xiong et al. (2020)](https://arxiv.org/pdf/2002.04745) compare pre-norm and post-norm architectures empirically. Read the abstract of their paper and summarise their main findings. According to these findings, what are the benefits of the pre-norm architecture?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Layer normalization (Ba et al., 2016) was moved to the input of each sub-block, similar to a pre-activation residual network.\n",
        "\n",
        " - Their theory shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. Its shown that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications"
      ],
      "metadata": {
        "id": "2CyU9S-rJEN9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BatTaRPxGbp_"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBDiKhA3GbqG"
      },
      "source": [
        "We now have almost all components in place to complete the implementation of the GPT-2 model. The only thing  missing are the position embeddings. These simply associate an embedding vector with every position in the context window. To set them up, we first define another utility function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QDmVF8JGbqG"
      },
      "outputs": [],
      "source": [
        "def make_positions(n):\n",
        "    return torch.arange(n, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3or2yyGGbqH"
      },
      "source": [
        "We then code the complete model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WpvkcIxGbqH"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.wte = nn.Embedding(config.n_vocab, config.n_embd)\n",
        "        self.wpe = nn.Embedding(config.n_ctx, config.n_embd)\n",
        "        self.h = nn.Sequential(*(Block(config) for _ in range(config.n_layer)))\n",
        "        self.ln_f = LayerNorm(config)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.n_vocab, bias=False)\n",
        "        self.register_buffer(\"pos\", make_positions(config.n_ctx), persistent=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.shape\n",
        "        wte = self.wte(x)\n",
        "        wpe = self.wpe(self.pos[:seq_len]) # type: ignore\n",
        "        x = wte + wpe\n",
        "        x = self.h(x)\n",
        "        x = self.ln_f(x)\n",
        "        x = self.lm_head(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl4UulmLGbqH"
      },
      "source": [
        "#### ðŸŽˆ Task 2.08: Buffers\n",
        "\n",
        "Our implementation registers the vector of positions as a buffer. (Earlier, we also registered the causal mask as a buffer.) Consult the PyTorch documentation to determine the benefits of registering a tensor as a buffer, in contrast to computing it in the `forward()` method."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These parametrs we want to save them and keep them as they are without any change, especially from the backpropagation, which keeps updating the paramters, however when it see the buffer it leaves it as it is. This is called **Non trainable**.\n",
        "So when we use them in the forward function, we only use their values and not changing their content. The problem is not exactly with changing it, but with the process of recomputing it over and over again on each pass which is resources consuming."
      ],
      "metadata": {
        "id": "lcipocpF9wwt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWVWNCETGbqH"
      },
      "source": [
        "#### ðŸŽ“ Task 2.09: Number of trainable parameters\n",
        "\n",
        "The model we have implemented is the smallest one presented by [Radford et al. (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). But how many trainable parameters exactly does it have? Interestingly, the number originally reported by the authors is wrong! What number did they report?\n",
        "\n",
        "Your task is to write code to compute the number of parameters yourself. This should only take 1â€“3 lines of code. What number do you get when you apply this code to a fresh model instance?\n",
        "\n",
        "[Radford et al. (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) followed the original Transformers paper ([Vaswani et al., 2017](https://doi.org/10.48550/arXiv.1706.03762)) and shared the trainable weights between the token embedding and the final linear layer. Implement this weight sharing strategy. (Hint: This only requires one line of code.) Then, re-compute the number of trainable parameters for the modified model. What number do you get now? How large is the reduction caused by the weight sharing?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The number of paramters is `117`\n",
        "model = Model(Config())\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of trainable parameters :{total_params}')\n",
        "model.lm_head.weight = model.wte.weight\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of trainable parameters for the modified model :{total_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS2ZgW1BamiA",
        "outputId": "14e28410-421a-4361-e84c-67783e118880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters :163037184\n",
            "Number of trainable parameters for the modified model :124439808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX52vHixGbqH"
      },
      "source": [
        "## Part 2: Load pre-trained weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKPQCJZrGbqI"
      },
      "source": [
        "Now that you have a complete implementation of the GPT-2 model in place, you can instantiate it by loading the pre-trained weights released by OpenAI. These weights were originally provided in the TensorFlow format. For this lab, we have re-packaged them as a single file in NumPyâ€™s `.npz` archive format. We can load it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_R52xp1GbqI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "pretrained = np.load(\"gpt-2-pretrained.npz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WORKgzf5GbqI"
      },
      "source": [
        "The result `pretrained` is a dictionary mapping names to NumPy arrays. When you print the names, you will see that they correspond to the attributes of our network modules, even though the names differ. For example, the array `h0.attn.c_attn.b` holds the biases (`b`) of the `c_attn` linear layer in the attention mechanism (`attn`) of the first transformer block (`h0`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G5jEZb2GbqI"
      },
      "source": [
        "#### ðŸŽ“ Task 2.10: Load pre-trained weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi1hzO3sGbqI"
      },
      "source": [
        "Create a model from the pre-trained weights. To do this, you need to instantiate a fresh model and write the contents of each array from the `npz` archive with the pre-trained weights into the corresponding tensor. To make this a bit easier, here is a utility function that copies data from a NumPy array `source` to a PyTorch tensor `target`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z6uU6HvGbqJ"
      },
      "outputs": [],
      "source": [
        "def copy_weights(source: np.ndarray, target: torch.Tensor):\n",
        "    assert source.shape == target.shape\n",
        "    with torch.no_grad():\n",
        "        target.copy_(torch.tensor(source, dtype=torch.float32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0iiPR_NGbqJ"
      },
      "source": [
        "You can start from this skeleton code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSwN7s2zGbqJ"
      },
      "outputs": [],
      "source": [
        "def from_pretrained() -> Model:\n",
        "    model = Model(Config())\n",
        "    pretrained = np.load(\"gpt-2-pretrained.npz\")\n",
        "    # TODO: Copy the weights from `pretrained` to `model`\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRw5pY8jGbqJ"
      },
      "source": [
        "**Important:** One technical detail to note is that PyTorch stores the weights of linear layers in a transposed form. For example, a linear layer created as `nn.Linear(2, 3)` has a weight matrix of shape [3, 2]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVXc-mImGbqJ"
      },
      "source": [
        "## Part 3: Put the model to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8zQQLBQGbqK"
      },
      "source": [
        "In the third and final part of this lab, you will use the pre-trained model to generate text and evaluate it on a standard benchmark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0gSxXcLGbqK"
      },
      "source": [
        "### Sampling-based text generation\n",
        "\n",
        "The easiest way to generate text with a language model is by using a **greedy approach**. This method works by creating text one token at a time. At each step, the model takes the previously generated text (called the **context**) as input and adds the token with the highest output logit as a new token. The code in the next cell defines a function `generate()` that forms the core of a greedy generator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htoOy71aGbqK"
      },
      "outputs": [],
      "source": [
        "def generate(model, context, context_size=1024, n_tokens=20):\n",
        "    for _ in range(n_tokens):\n",
        "        context = context[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(context)[:, -1, :]\n",
        "        next_idx = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "        context = torch.cat([context, next_idx], dim=-1)\n",
        "    return context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wst1MSY-GbqK"
      },
      "source": [
        "To use this function with an actual text input, you need a tokeniser to first encode the text into a vector of token IDs, and later decode the generated `context` into new text. The reference implementation of the GPT-2 tokeniser is in the library `tiktoken`. The code in the next cell sets up the tokeniser, loads the pretrained model from Task&nbsp;2.10, and then defines a helper function that handles the encoding and decoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBggKvqSGbqK"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "model = from_pretrained()\n",
        "\n",
        "\n",
        "def generate_helper(text, context_size=1024, n_tokens=20):\n",
        "    context = torch.tensor([tokenizer.encode(text)], dtype=torch.long)\n",
        "    context = generate(model, context, context_size=context_size, n_tokens=n_tokens)\n",
        "    return tokenizer.decode(context[0].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj2xV8uFGbqL"
      },
      "source": [
        "You can use this helper function to generate text as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGwHfGbLGbqL"
      },
      "outputs": [],
      "source": [
        "generate_helper(\"LinkÃ¶ping University is\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDYWTS_sGbqL"
      },
      "source": [
        "**Tip:** If you did not manage to complete Task&nbsp;2.10, you can still work on this task by using a pretrained GPT-2 model from [Hugging Face](https://huggingface.co/openai-community/gpt2). The next code cell shows how you would instantiate this model. Note that you may have to first install the `transformers` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Nh3TnL5GbqL"
      },
      "outputs": [],
      "source": [
        "# from transformers import GPT2LMHeadModel\n",
        "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "# logits = model(context).logits[:, -1, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM3_cDr0GbqM"
      },
      "source": [
        "#### ðŸŽ“ Task 2.11: Sampling-based text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mQdO8AXGbqM"
      },
      "source": [
        " The greedy approach to text generation is not very interesting for practical applications because it always chooses the most likely token, leading to predictable and less creative results. Your task is to modify the code for the `generate()` function to use a **sampling-based approach** instead. In this approach, the next token is chosen randomly based on the probabilities assigned by the model (softmax-normalised logits), treating them as a categorical distribution over the token vocabulary. Additionally, your code should include two common techniques to improve sampling:\n",
        "\n",
        " * **temperature scaling**, which lets the user control the randomness of the sampling\n",
        " * **top-$k$ sampling**, which limits the sampling to the top-$k$ most likely tokens, ignoring less probable ones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITD0cAUaGbqM"
      },
      "source": [
        "### Evaluating the pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E3-Dt9RGbqM"
      },
      "source": [
        "If you have experimented with your pretrained GPT-2 model, you will have noticed that its ability to generate useful text is somewhat limited. By todayâ€™s standards, GPT-2 is a small model with modest capabilities. However, it can still be helpful for certain tasks, such as text autocompletion, generating filler text, or answering simple questions. To rigourosly evaluate language models, researchers often use standard benchmark datasets. Creating these benchmarks is a discipline of its own, and they tend to become increasingly challenging as models continue to improve.\n",
        "\n",
        "In the final task of this lab, you will evaluate GPT-2â€™s performance on a small subset of the [HellaSwag dataset](https://rowanzellers.com/hellaswag/), which was published in the same year as GPT-2 itself (2019). HellaSwag is designed to test a modelâ€™s ability to perform commonsense reasoning in challenging contexts. Unlike simpler benchmarks, HellaSwag presents scenarios where the correct text completion depends on semantic relationships between events and on world knowledge. This makes it a good choice for assessing the ability of language models to go beyond surface-level patterns and produce meaningful, context-aware predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPkbd8WNGbqM"
      },
      "source": [
        "#### ðŸŽ“ Task 2.12: Evaluating the pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMJnFho5GbqM"
      },
      "source": [
        "Read the [HellaSwag website](https://rowanzellers.com/hellaswag/) to get some background on the benchmark. How does a sample from the dataset look like? What is an expected prediction? How does the benchmark allow us to score models? What is the random baseline? What is the human performance reported on the task?\n",
        "\n",
        "The next cell contains code for evaluating your pretrained model on a small sample from HellaSwag. You will also need a tokenizer. The HellaSwag subset is in the file `hellaswag-mini.jsonl`. Inspect that file to understand the format. Next, read the code and explain how it works. Specifically, how does the code compute the score of individual endings? In the call to `cross_entropy()`, why are the tensors sliced in this specific way?\n",
        "\n",
        "Finally, what overall score does the pretrained GPT-2 model get on this benchmark? How does that score compare to the random baseline and the human performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIGTOtupGbqN"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"hellaswag-mini.jsonl\") as f:\n",
        "    n_correct = 0\n",
        "    n_total = 0\n",
        "    for line in f:\n",
        "        sample = json.loads(line)\n",
        "        prefix = tokenizer.encode(sample[\"ctx\"])\n",
        "        ending_scores = []\n",
        "        for i, ending in enumerate(sample[\"endings\"]):\n",
        "            suffix = tokenizer.encode(\" \" + ending)\n",
        "            context = torch.tensor([prefix + suffix], dtype=torch.long)\n",
        "            with torch.no_grad():\n",
        "                logits = model(context)\n",
        "                ending_score = torch.nn.functional.cross_entropy(\n",
        "                    logits[0, -len(suffix) - 1 : -1], context[0, -len(suffix) :]\n",
        "                )\n",
        "            ending_scores.append((ending_score, i))\n",
        "        predicted = min(ending_scores)[1]\n",
        "        n_correct += int(predicted == sample[\"label\"])\n",
        "        n_total += 1\n",
        "    print(f\"Accuracy: {n_correct / n_total:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6OuzJ3zGbqN"
      },
      "source": [
        "**ðŸ¥³ Congratulations on finishing lab&nbsp;2!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}