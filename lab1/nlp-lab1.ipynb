{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Tokenisation and embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will build an understanding of how text can be transformed into representations that computers can process and learn from. Specifically, you will explore two key concepts: *tokenisation* and *embeddings*. Tokenisation splits text into smaller units such as words, subwords, or characters. Embeddings are dense, fixed-size vector representations of tokens in a continuous space.\n",
    "\n",
    "*Tasks you can choose for the oral exam are marked with the graduation cap ðŸŽ“ emoji.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the lab, you will code and analyse a tokeniser based on the Byte Pair Encoding (BPE) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BPE tokeniser transforms text into a list of integers representing tokens. As a warm-up, you will implement two utility functions on such lists. To simplify things, we define a shorthand for the type of pairs of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "Pair = Tuple[int, int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽˆ Task 1.01: Counting pairs\n",
    "\n",
    "Write a function that counts all occurrences of pairs of consecutive token IDs in a given list. The function should return a dictionary that maps each pair to its count. Skip counts that are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(ids: list[int]):\n",
    "    result = {}\n",
    "    for i in range(len(ids) - 1):\n",
    "        if (ids[i], ids[i + 1]) in result:\n",
    "            result[(ids[i] , ids[i+1])] += 1\n",
    "        else:\n",
    "            result[(ids[i] , ids[i+1])] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 2): 2, (2, 2): 1, (2, 3): 1, (3, 1): 1, (2, 1): 1}\n"
     ]
    }
   ],
   "source": [
    "print(count([1, 2, 2, 3, 1, 2, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽˆ Task 1.02: Replacing pairs\n",
    "\n",
    "Write a function that traverses a list of token IDs from left to right and replaces all occurrences of a specified pair of consecutive IDs by a new ID. The function should return the modified list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(ids: list[int], pair: Pair, new_id: int) -> list[int]:\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(ids) - 1 and len(ids) > 0:\n",
    "        if pair == (ids[i], ids[i+1]):\n",
    "            result.append(new_id)\n",
    "            i += 2\n",
    "        else:\n",
    "            result.append(ids[i])\n",
    "            i += 1\n",
    "    if len(ids) > 2:\n",
    "        result.append(ids[len(ids) - 1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains the core code for the tokeniser in the form of a class `Tokenizer`. This class implements two methods: `encode()` converts an input text to a list of token IDs by exhaustively applying rules for merging pairs of consecutive IDs (stored in the dictionary `self.merges`), and `decode()` reverses this process. Note that the set of merge rules is initially empty; you will add rules in Task&nbsp;1.04."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.merges: dict[Pair, int] = {}\n",
    "        self.vocab: dict[int, bytes] = {i: bytes([i]) for i in range(2**8)} \n",
    "\n",
    "    def encode(self, text):\n",
    "        ids: list[int] = list(text.encode(\"utf-8\")) # String is converted into a list of integers \"ab\" ==> [97, 98]\n",
    "        while True:\n",
    "            counts:dict[Pair, int] = count(ids) # {(97, 98): 3, (98, 97): 2}\n",
    "            mergeable_pairs: set[Pair] = counts.keys() & self.merges.keys() # Check for matches tokens, ex merges = {(97, 98): 257, (98, 97): 258}\n",
    "            if len(mergeable_pairs) == 0:\n",
    "                break\n",
    "            to_merge: Pair = min(mergeable_pairs, key=self.merges.get)  # take the token with lower ID ==> (97, 98)\n",
    "            ids: list[int] = replace(ids, to_merge, self.merges[to_merge])  # Replace the token with the merged token\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return b\"\".join((self.vocab[i] for i in ids)).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽ“ Task 1.03: Encoding and decoding\n",
    "\n",
    "Explain how the code implements the BPE algorithm. Use the following steps to check your understanding:\n",
    "\n",
    "**Step&nbsp;1.** Annotate the attributes and methods of the `Tokenizer` class with their Python types. In particular, what is the type of `self.merges`? Use the `Pair` shorthand. What does a merge rule look like?\n",
    "\n",
    "**Step&nbsp;2.** Explain how the implementation chooses which merge rule to apply. Provide an example that illustrates the logic. Construct the example such that you get a different result when you use `max()` instead of `min()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The self.merges return a tuple with id, this id will be used later as the tuple that will be selected based on (Min or Max).\n",
    "In the comment I explained with an example of which tuple might be selected with the current code, in case of Max then (98, 97) will be selected since ID 258 is higher than ID 257 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon initialisation, a tokeniser has an empty set of merge rules. Your next task is to complete the BPE algorithm and write code to learn these merge rules from a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽ“ Task 1.04: Training a tokeniser\n",
    "\n",
    "Write a function that induces a BPE tokeniser from a given text. The function should take the text (a string) and a target vocabulary size as input and return the trained tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_text(text: str, vocab_size: int) -> Tokenizer:\n",
    "    tok = Tokenizer()\n",
    "    ids = list(text.encode(\"utf-8\"))\n",
    "    while len(tok.vocab) < vocab_size:\n",
    "        counts = count(ids)\n",
    "        if not counts:\n",
    "            break\n",
    "        selected_pair = max(counts, key=counts.get)\n",
    "        token_id = len(tok.vocab)\n",
    "        tok.merges[selected_pair] = token_id\n",
    "        tok.vocab[token_id] = tok.vocab[selected_pair[0]] + tok.vocab[selected_pair[1]]\n",
    "        ids = replace(ids, selected_pair, token_id)\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you test your implementation, we provide three text files together with tokenisers trained on these files. Each text file contains the first 1&nbsp;million Unicode characters in a language-specific Wikipedia:\n",
    "\n",
    "| Text file | Tokeniser file | Wikipedia |\n",
    "|---|---|---|\n",
    "| `wiki-en-1m.txt` | `wiki-en-1m.tok` | [Simple English](https://simple.wikipedia.org/) |\n",
    "| `wiki-is-1m.txt` | `wiki-is-1m.tok` | [Icelandic](https://is.wikipedia.org/) |\n",
    "| `wiki-sv-1m.txt` | `wiki-sv-1m.tok` | [Swedish](https://sv.wikipedia.org/) |\n",
    "\n",
    "A tokeniser file consists of lines specifying merge rules. For example, the first line in the tokeniser file for Swedish is `101 114`, which expresses that this rule combines the token with ID 101 (`e`) and the token with ID 114 (`r`). The ID of the new token (`er`) is 256 plus the (zero-indexed) line number on which the rule is found. The following code saves a `Tokenizer` to a file with this format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(tokenizer: Tokenizer, filename: str) -> None:\n",
    "    with open(filename, \"w\") as f:\n",
    "        for fst, snd in tokenizer.merges:\n",
    "            print(f\"{fst} {snd}\", file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n",
      "<__main__.Tokenizer object at 0x000001FCB3EE4850>\n"
     ]
    }
   ],
   "source": [
    "with open('wiki-sv-1m.txt', 'r', encoding='utf-8') as f:\n",
    "    text_content = f.read()\n",
    "\n",
    "my_tok = from_text(text_content, 1256)\n",
    "\n",
    "save(my_tok, 'my_wiki_swedish.tok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your code, compare your saved tokeniser to the provided tokeniser using the `diff` tool.\n",
    "\n",
    "**Note that training a tokeniser can take a few minutes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation quirks\n",
    "\n",
    "The tokeniser is a key component of language models, as it defines the minimal chunks of text the model can â€œseeâ€ and work with. As you will see in this section, tokenisation is also responsible for several deficiencies and unexpected behaviours of language models.\n",
    "\n",
    "One helpful tool for experimenting with tokenisers in language models is the web app [Tiktokenizer](https://tiktokenizer.vercel.app/). This app lets you play around with, among others, [`cl100k_base`](https://tiktokenizer.vercel.app/?model=cl100k_base), the tokeniser used in the free version of ChatGPT and OpenAIâ€™s APIs, and [`o200k_base`](https://tiktokenizer.vercel.app/?model=o200k_base), used in GPT-4o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽ“ Task 1.05: Tokenisation quirks\n",
    "\n",
    "Prompt [ChatGPT](https://chatgpt.com/) to reverse the letters in the following words:\n",
    "\n",
    "```\n",
    "creativecommons\n",
    "MERCHANTABILITY\n",
    "NSNotification\n",
    "authentication\n",
    "```\n",
    "\n",
    "How many of these words come out right? What happens when you modify the prompt and explicitly disable â€œthinkingâ€ and external tools? What could be the problem when words come out wrong? Generate ideas by inspecting the words in Tiktokenizer. Try to come up with other prompts that illustrate problems related to tokenisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result I got: \n",
    "\n",
    "`creativecommons â†’ smonivtaerceerc`  âŒ\n",
    "\n",
    "`MERCHANTABILITY â†’ YTILIBAREDNOMMOC` âŒ\n",
    "\n",
    "`NSNotification â†’ noitacifitoSN` âŒ\n",
    "\n",
    "`authentication â†’ noitacitnehtua` âœ…\n",
    "\n",
    "My word `interpretability â†’ ytilibatrepretni `âŒ\n",
    "\n",
    "Only `authentication` was correctly reversed, the result was acheived without using (thinking nor external tools options).\n",
    "\n",
    "I belive the reason behind it is that the reversed word is not something common, which means it is meaningless words therfore when working with word `creativecommons` it starts with `s` and then the predicted character that most commonly after `s` is `m`. Hence, the reversed word is most likely wrong.\n",
    "By using the Tiktokenizer this word `creativecommons` was analyzed as a whole and not subdivided into subwords.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation and multi-linguality\n",
    "\n",
    "Many NLP systems and the tokenisers used with them are primarily trained on English data. In the next task, you will reflect on the effect this has when they are used to process non-English data.\n",
    "\n",
    "The *context length* of a language model is the maximum number of preceding tokens the model can condition on when predicting the next token. This number is fixed and cannot be changed after training the model. For example, the context length of GPT-2 ([Radford et al., 2019](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)) is 1,024. \n",
    "\n",
    "While the context length of a language model is fixed, the amount of information that can be squeezed into this context length will depend on the tokeniser. Informally speaking, a model that needs more tokens to represent a given text cannot extract as much information from that text as one that needs fewer tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽ“ Task 1.06: Tokenisation and multi-linguality\n",
    "\n",
    "Train a tokeniser on the English text file from Task&nbsp;1.04 and test it on the same text. How many tokens does it split the text into? Based on this, what is the expected number of Unicode characters of English text that can be fit into a context length of 1,024?\n",
    "\n",
    "What do the numbers look like if you test the English tokeniser on the Icelandic text instead? What could explain the differences?\n",
    "\n",
    "Interpreting the expected number of Unicode characters as a measure of representation efficiency, what do your results tell you about the efficiency of a language model primarily trained on English data when it is used to process non-English data? Why are these findings relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki-en-1m.txt', 'r', encoding='utf-8') as f:\n",
    "    text_content_en = f.read()\n",
    "\n",
    "my_tok_english = from_text(text_content_en, 1256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "739566\n"
     ]
    }
   ],
   "source": [
    "with open('wiki-is-1m.txt', 'r', encoding='utf-8') as f:\n",
    "    text_content_is = f.read()\n",
    "    \n",
    "encoded_text = my_tok_english.encode(text_content_is)\n",
    "print(len(encoded_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The number of tokens is : `355650`\n",
    "\n",
    "Let's check the number of characters per token.\n",
    "`1000000/ 355650 = 2.81` \n",
    "\n",
    "The number of Unicode characters of English text that can be fit into a context of length 1024 would be \n",
    "\n",
    "\n",
    "`1024/ 2.81 = 364`\n",
    "\n",
    "- Testing on Icelandic language: \n",
    "\n",
    "The number of token is : `739566`\n",
    "\n",
    "Let's check the number of characters per token.\n",
    "`1000000/ 739566 = 1.35` \n",
    "\n",
    "The number of Unicode characters of Icelandc text that can be fit into a context of length 1024 would be \n",
    "\n",
    "`1024 / 1.35 = 758 ` \n",
    "\n",
    "My interpretation is : The Icelandic language has more characters than English, thus, the number of tokens is higher than the one in English text, more tokens means more new words/subwords and less repetitive words/subwords. Additionally, words that contains non-English characters will end up creating more tokens since splitting will be small, and they don't have a meaninig in the English context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part of the lab, you will explore embeddings. An embedding layer is a network component that assigns each item in a finite set of elements (often called a *vocabulary*) a fixed-size vector. At first, these vectors are filled with random values, but during training, they are adjusted to suit the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you build an intuition for embeddings and the vector representations learned by them, we will use a simple bag-of-words text classifier. The core part of this classifier only takes a few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(self.embedding(x).mean(dim=-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽˆ Task 1.07: Bag-of-words classifier\n",
    "\n",
    "Explain how the bag-of-words classifier works. How does the code match the diagram you saw in the lectures? Why is there only one `nn.Embedding`, while the diagram shows three embedding layers? What does the keyword argument `dim=-2` do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a way for representing the words of a text by writing the number of occurences of each word. Each embedding layer represent a word to embed, so `num_embeddings` is basically the number of words to embed. \n",
    "\n",
    "The keyword `-2` represent dimensionality, the negative means count from the end, and 2 in this case 2 dimensions, thus, it will be the second dimensions from the end until the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will apply the classifier to a small dataset with Amazon customer reviews. This dataset is taken from [a much larger dataset](https://www.cs.jhu.edu/~mdredze/datasets/sentiment/) first described by [Blitzer et al. (2007)](https://aclanthology.org/P07-1056/).\n",
    "\n",
    "The dataset contains whitespace-tokenised product reviews from two categories: cameras (`camera`) and music (`music`). Each review is additionally annotated for sentiment towards the product at hand: negative (`neg`) or positive (`pos`). The category and sentiment labels are prepended to the review. As an example, here is the first review from the training data:\n",
    "\n",
    "```\n",
    "music neg oh man , this sucks really bad . good thing nu-metal is dead . thrash metal is real metal , this is for posers\n",
    "```\n",
    "\n",
    "The next cell contains a custom [`Dataset`](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) class for the review dataset. To initialise an instance of this class, you specify the name of the file containing the reviews you want to load (`filename`) and which of the two labels you want to use (`label`): product category (0) or sentiment (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, filename: str, label: int = 0) -> None:\n",
    "        with open(filename) as f:\n",
    "            tokenized_lines = [line.split() for line in f]\n",
    "        self.items = [(tokens[2:], tokens[label]) for tokens in tokenized_lines]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[list[str], str]:\n",
    "        return self.items[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectoriser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed a review into the bag-of-words classifier, you first need to turn it into a vector of token IDs. Likewise, you need to convert the label (product category or sentiment) into an integer. The next cell contains a partially completed `ReviewVectoriser` class that handles this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "\n",
    "# Type abbreviation for reviewâ€“label pairs\n",
    "Item = tuple[list[str], str]\n",
    "\n",
    "\n",
    "class ReviewVectorizer:\n",
    "    PAD = \"[PAD]\"\n",
    "    UNK = \"[UNK]\"\n",
    "\n",
    "    def __init__(self, dataset: ReviewDataset, n_vocab: int = 1024) -> None:\n",
    "        # Unzip the dataset into reviews and labels\n",
    "        reviews, labels = zip(*dataset)\n",
    "\n",
    "        # Count the tokens and get the most common ones\n",
    "        counter = Counter(t for r in reviews for t in r)\n",
    "        most_common = [t for t, _ in counter.most_common(n_vocab - 2)] # -2 to reserve PAD and UNK\n",
    "\n",
    "        # Create the token-to-index and label-to-index mappings\n",
    "        self.t2i = {t: i for i, t in enumerate([self.PAD, self.UNK] + most_common)}\n",
    "        self.l2i = {l: i for i, l in enumerate(sorted(set(labels)))}\n",
    "\n",
    "    def __call__(self, items: list[Item]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        reviews, labels = zip(*items)\n",
    "        longest_review = max(len(review) for review in reviews)\n",
    "        X = []\n",
    "        # Now we need to add the padding to smaller reviews\n",
    "\n",
    "        for review in reviews:\n",
    "            token_ids = []\n",
    "            while len(review) < longest_review:\n",
    "                review.append(self.t2i[self.PAD])\n",
    "            for token in review:\n",
    "                if token in self.t2i:\n",
    "                    token_ids.append(self.t2i[token])\n",
    "                else:\n",
    "                    token_ids.append(self.t2i[self.UNK])\n",
    "            X.append(token_ids)\n",
    "        y = [self.l2i[label] for label in labels]\n",
    "        return torch.tensor(X, dtype=torch.long), torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `ReviewVectoriser` maps tokens and labels to IDs using two Python dictionaries. These dictionaries are set up when the vectoriser is initialised and queried when the vectoriser is called on a batch of reviewâ€“label pairs. They include IDs for two special tokens:\n",
    "\n",
    "`[PAD]` (Padding): Reviews can have different lengths, but PyTorch requires all vectors in a batch to be the same size. To handle this, the vectoriser adds `[PAD]` tokens to the end of shorter reviews so they match the length of the longest review in the batch.\n",
    "\n",
    "`[UNK]` (Unknown): If a review contains a token that is not in the token-to-ID dictionary, the vectoriser assigns it the ID of the `[UNK]` token instead of a regular ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽ“ Task 1.08: Vectoriser\n",
    "\n",
    "Explain and complete the code of the vectoriser. Follow these steps:\n",
    "\n",
    "**Step&nbsp;1.** Explain how unzipping works. What are the types of `reviews` and `labels`?\n",
    "\n",
    "**Step&nbsp;2.** Explain how the token-to-ID and label-to-ID mappings are constructed. How does the `most_common()` method deal with elements that occur equally often?\n",
    "\n",
    "**Step&nbsp;3.** Complete the implementation of the `__call__()` method. This method should convert a list of $m$ reviewâ€“label pairs into a pair $(X, y)$ where $X$ is a matrix containing the vectors with token IDs for the reviews, and $y$ is a vector containing the IDs of the corresponding labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1_ Unzipping means separating the passed iterable pairs into individual iterable of items for each, in this example we are separating the reviews and the labels. the types of reviews and labels: list of string for reviews, and string for labels\n",
    "\n",
    "- 2_ First we count the number of tokens from each review, then we count the most common token by calling the most common method, this method return list of token count in descending order, in case of ties => selecting will be arbitrary (the order does not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the vectoriser completed, you are ready to train a classifier. More specifically, you can train two separate classifiers: one to predict the product category of a review, and one to predict the sentiment. The next cell contains a simple training loop that you can adapt for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "file_name = \"reviews-train.txt\"\n",
    "n_vocab: int = 1024\n",
    "def train(label):\n",
    "    dataset = ReviewDataset(file_name, label)  # The dataset used and the selected label\n",
    "    processor = ReviewVectorizer(dataset, n_vocab)  # The vectorizer used to transform the raw text into a vector that the machine can use.\n",
    "    num_embeddings: int = 1024 # the size of our vocabulary. It's the total number of unique words that the model can recognize and work with.\n",
    "    embedding_dim: int = 64 # the size of the vector that represents each of those words\n",
    "    num_classes: int = len(processor.l2i) # the number of classes the model is trying to predict\n",
    "    learning_rate = 0.001 # How much the model's paramters are updated during training steps.\n",
    "    batch_size = 16 # The number of datasamples trained at once\n",
    "    num_of_epochs = 10 # Number of epochs which is the complete pass over the entire dataset\n",
    "    model = Classifier(num_embeddings, embedding_dim, num_classes)  # The model used which is the bag-of-words classifier\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate)  # The optimzer Adam (It doesn't use the same learning rate. Instead, it adjusts the learning rate for each parameter.)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=processor,\n",
    "    )\n",
    "    for epoch in range(num_of_epochs):  \n",
    "        model.train()  # puts the model into training mode\n",
    "        running_loss = 0  \n",
    "        for bx, by in data_loader:\n",
    "            optimizer.zero_grad() # Reseting the gradient, this is useful to avoid having a prior information each iteration. without reseting the model wouldn't learn correctly.\n",
    "            output = model(bx) # is the prediction of the model\n",
    "            loss = F.cross_entropy(output, by) # it shows how far off the model's predictions are from the correct answers.\n",
    "            loss.backward() # Backpropagation which is used to reduce the loss\n",
    "            optimizer.step() # Takes the gradients and use it to update the model's paramters. ( this is the actual learning happen)\n",
    "            running_loss += loss.item() # This is just accumulating the losses from each iteration then use them all to show the final loss.\n",
    "        print(f\"Epoch {epoch}, loss: {running_loss / len(data_loader):.4f}\")\n",
    "    return processor, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽ“ Task 1.09: Training loop\n",
    "\n",
    "Explain the training loop. Follow these steps:\n",
    "\n",
    "**Step&nbsp;1.** Go through the training loop line-by-line and add comments where you find it suitable. Your comments should be detailed enough for you to explain the main steps of the loop.\n",
    "\n",
    "**Step&nbsp;2.** The training loop contains various hard-coded values like filename, learning rate, batch size, and epoch count. This makes the code less flexible. Revise the code so that you can specify these values using keyword arguments. Use the concrete values from the code as defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽˆ Task 1.10: Training the classifier\n",
    "\n",
    "Adapt the next cell to train the classifier for the two prediction tasks. Based on the loss values, which task appears to be the harder one? What is the purpose of setting a seed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss for the Category\n",
      "Epoch 0, loss: 0.6814\n",
      "Epoch 1, loss: 0.6506\n",
      "Epoch 2, loss: 0.6077\n",
      "Epoch 3, loss: 0.5397\n",
      "Epoch 4, loss: 0.4670\n",
      "Epoch 5, loss: 0.3934\n",
      "Epoch 6, loss: 0.3332\n",
      "Epoch 7, loss: 0.2856\n",
      "Epoch 8, loss: 0.2471\n",
      "Epoch 9, loss: 0.2177\n",
      "The loss for the Sentiment\n",
      "Epoch 0, loss: 0.6900\n",
      "Epoch 1, loss: 0.6840\n",
      "Epoch 2, loss: 0.6825\n",
      "Epoch 3, loss: 0.6723\n",
      "Epoch 4, loss: 0.6615\n",
      "Epoch 5, loss: 0.6467\n",
      "Epoch 6, loss: 0.6298\n",
      "Epoch 7, loss: 0.6096\n",
      "Epoch 8, loss: 0.5895\n",
      "Epoch 9, loss: 0.5688\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "print('The loss for the Category')\n",
    "vectorizer, model = train(0)\n",
    "print()\n",
    "print('The loss for the Sentiment')\n",
    "vectorizer2, model2 = train(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The seed is needed to get the same random values we got first time time when training our model, to avoid having different output each time we train the model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have trained the classifier on two separate prediction tasks, it is interesting to inspect and compare the embedding vectors it learned in the process. For this you will use an online tool called the [Embedding Projector](http://projector.tensorflow.org). The next cell contains code to save the embeddings from a trained classifier in a format that can be loaded into this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(\n",
    "    vectorizer: ReviewVectorizer,\n",
    "    model: Classifier,\n",
    "    vectors_filename: str,\n",
    "    metadata_filename: str,\n",
    "):\n",
    "    i2t = {i: t for t, i in vectorizer.t2i.items()}\n",
    "    embeddings = model.embedding.weight.detach().numpy()\n",
    "    items = [(i2t[i], e) for i, e in enumerate(embeddings)]\n",
    "    with open(vectors_filename, \"wt\") as f1, open(metadata_filename, \"wt\") as f2:\n",
    "        for w, e in items:\n",
    "            print(\"\\t\".join(\"{:.5f}\".format(x) for x in e), file=f1)\n",
    "            print(w, file=f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call this code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings(vectorizer, model, \"vectors.tsv\", \"metadata.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽ“ Task 1.11: Inspecting the embeddings\n",
    "\n",
    "Load the embeddings from the two classification tasks (product category classification and sentiment classification) into the Embedding Projector web app and inspect the vector spaces. How do they compare visually? Does the visualisation make sense to you?\n",
    "\n",
    "The Embedding Projector offers visualisations based on three dimensionality reduction methods: [UMAP](https://umap-learn.readthedocs.io/en/latest/), [T-SNE](https://en.m.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding), and [PCA](https://en.m.wikipedia.org/wiki/Principal_component_analysis). Which of these seems most useful to you?\n",
    "\n",
    "Focus on the embeddings for the words *repair* and *sturdy*. Are they close to each other or far away from another? What happens if you switch to the other task? How do you explain that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation of embedding layers\n",
    "\n",
    "The error surfaces explored when training neural networks can be very complex. Because of this, it is crucial to choose â€œgoodâ€ initial values for the parameters. In the final task of this lab, you will run a small experiment to see how alternative initialisations can affect a modelâ€™s performance.\n",
    "\n",
    "In PyTorch, the weights of the embedding layer are initially set by sampling from the standard normal distribution, $\\mathcal{N}(0, 1)$. However, research suggests other approaches may work better. For example, given that embedding layers share similarities with linear layers, it makes sense to use the same initialisation method for both. The default initialisation method for linear layers in PyTorch is the so-called Kaiming initialisation, introduced by [He et al. (2015)](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽˆ Task 1.12: Initialisation of embedding layers\n",
    "\n",
    "Check the [source code of `nn.Linear`](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear) to see how PyTorch initialises the weights of linear layers using the Kaiming initialisation method. Apply the same method to the embedding layer of your classifier and see how this affects the loss of your model and the vector spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ¥³ Congratulations on finishing lab&nbsp;1!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
